import timeimport torchfrom torch import nnfrom einops import rearrangefrom Dim.PCA import spectral_pca_reductionfrom Train_and_Eval.device import get_device# @torch.compile(mode="reduce-overhead")class AllinMamba(nn.Module):    def __init__(self, input_channels, num_classes, patch_size, feature_dim=128, depth=1, mlp_dim=64, dropout=0.37,                 d_state=16, expand=8, mode=2, root_mamba=False):        super().__init__()        self.dim = 2        self.feature_dim = feature_dim        self.depth = depth        self.patch_size = patch_size        self.dropout = dropout        self.scan_length = patch_size // 2 * 4 + 4        self.input_channels = input_channels        self.chunk_size = 4        self.conv3d_sep = nn.Sequential(            nn.Conv3d(in_channels=1, out_channels=1, kernel_size=(7, 3, 3), padding=(3, 1, 1), groups=1),            nn.BatchNorm3d(1),            nn.SiLU(),            nn.Conv3d(in_channels=1, out_channels=8, kernel_size=1),            nn.BatchNorm3d(8),            nn.SiLU()        )        self.conv2d_sep = nn.Sequential(            nn.Conv2d(in_channels=input_channels * 8, out_channels=input_channels * 8, kernel_size=3, padding=1,                      groups=input_channels * 8),            nn.BatchNorm2d(input_channels * 8),            nn.ReLU(),            nn.Conv2d(in_channels=input_channels * 8, out_channels=feature_dim, kernel_size=1),            nn.BatchNorm2d(feature_dim),            nn.ReLU()        )        self.conv2d_dim = nn.Sequential(            nn.Conv2d(in_channels=input_channels, out_channels=feature_dim, kernel_size=1),            nn.ReLU()        )        self.linear = nn.Linear(input_channels, feature_dim)        self.fusion_gate = nn.Sequential(            nn.Linear(self.feature_dim * 2, 2),            nn.Softmax(dim=1)        )        self.classifier = nn.Sequential(            nn.Linear(feature_dim, mlp_dim),            nn.ReLU(),            nn.Dropout(dropout),            nn.Linear(mlp_dim, num_classes),            nn.BatchNorm1d(num_classes)        )        if root_mamba:            try:                from mamba_ssm import Mamba2, Mamba as Mamba1            except ImportError:                from src.MambaBase.Mamba1 import Mamba1                from src.MambaBase.Mamba2 import Mamba2        else:            from src.MambaBase.Mamba1 import Mamba1            from src.MambaBase.Mamba2 import Mamba2        # d_conv = 2/4        if mode == 1:            self.MambaLayer1 = Mamba1(                d_model=feature_dim,                d_state=d_state,                expand=expand            )            self.MambaLayer2 = Mamba1(                d_model=self.scan_length,                d_state=d_state,                expand=expand            )            self.MambaLayer3 = Mamba1(                d_model=self.input_channels,                d_state=d_state,                expand=expand            )        else:            # head 数量 = expand * d_model/ headdim。            self.MambaLayer1 = Mamba2(                d_model=feature_dim,                d_state=d_state,                headdim=16,                expand=expand,                chunk_size=self.chunk_size            )            self.MambaLayer2 = Mamba2(                d_model=self.scan_length,                d_state=d_state,                headdim=16,                expand=expand,                chunk_size=self.chunk_size            )            self.MambaLayer3 = Mamba2(                d_model=self.input_channels,                d_state=d_state,                headdim=16,                expand=expand,                chunk_size=1            )    @staticmethod    def prepare_data_standard(x):        """        对输入张量进行标准化处理        Args:            x: 输入张量 (B, C, H, W)        Returns:            标准化后的张量 (B, C, H, W)        """        # 计算每个通道的均值和标准差，保持维度便于广播        mean = x.mean(dim=(0, 2, 3), keepdim=True)        std = x.std(dim=(0, 2, 3), keepdim=True) + 1e-6  # 防止除零        # 标准化操作        x_normalized = (x - mean) / std        return x_normalized    def feature_extraction_baseline(self, x):        """        Baseline 实现：使用 conv3d_sep + conv2d_sep 提取特征，        输入: x，形状 (B, C, H, W)        输出: 特征张量, 形状 (B, feature_dim, H, W)        """        x_out = self.conv3d_sep(x.unsqueeze(1))        x_out = rearrange(x_out, 'b t c h w -> b (t c) h w')        x_out = self.conv2d_sep(x_out)        return x_out    @staticmethod    def get_weights(length, device='cuda', sigma=1.0):        center = length // 2        indices = torch.arange(length, device=device)        squared_distances = (indices - center) ** 2 + center ** 2        decay_factor = -0.5 / (sigma ** 2)        weights = torch.exp(decay_factor * squared_distances)        weights = weights / weights.sum()        return weights    def weight_scan(self, x):        """优化版螺旋扫描函数"""        center = self.patch_size // 2        batch_size, channels = x.shape[0], x.shape[1]        x_scan = torch.zeros((batch_size, channels, self.scan_length), device=x.device)        x_scan[:, :, 0] = x[:, :, center, center]        # 预计算权重        weights_cache = {}        for i in range(center):            size = 2 * i + 3            if size not in weights_cache:                weights_cache[size] = self.get_weights(size, device=x.device)            weights = weights_cache[size]            weights_sum = weights.sum()  # 只计算一次            # 顶行、右列、底行、左列的扫描和加权平均            slices = [                x[:, :, center - i - 1, center - i - 1:center + i + 2],  # 顶行                x[:, :, center - i - 1:center + i + 2, center + i + 1],  # 右列                x[:, :, center + i + 1, center - i - 1:center + i + 2],  # 底行                x[:, :, center - i - 1:center + i + 2, center - i - 1]  # 左列            ]            for j, slice_data in enumerate(slices):                x_scan[:, :, i * 4 + 4 + j] = (slice_data * weights.view(1, 1, -1)).sum(dim=-1) / weights_sum        return x_scan.transpose(1, 2)    def MambaBlock(self, x):        """        Args:            x: 输入张量, shape=(B, scan_length, hidden_dim)        Returns:            经过MambaBlock处理后的张量, shape=(B, scan_length, hidden_dim)        """        if self.depth == 1:            # 单层直接处理，避免循环开销            x = self.MambaLayer1(x)            x = x.transpose(1, 2)            x = self.MambaLayer2(x)            return x.transpose(1, 2)        else:            for _ in range(self.depth):                x = self.MambaLayer1(x)                x = x.transpose(1, 2)                x = self.MambaLayer2(x)                x = x.transpose(1, 2)            return x    def spatial_route(self, x):        x = self.feature_extraction_baseline(x)        x = self.weight_scan(x)        x = self.MambaBlock(x)        x = x[:, 0:4].mean(dim=1)        return x    def spectral_route(self, x):        center_pixel = x[:, :, self.patch_size // 2, self.patch_size // 2]        center_pixel = center_pixel.unsqueeze(1)        for _ in range(self.depth):            center_pixel = self.MambaLayer3(center_pixel)        center_pixel = self.linear(center_pixel.squeeze(1))        return center_pixel    @staticmethod    def fusion_way_baseline(x1, x2):        gate = torch.sigmoid(x2)        x1 = gate * x1        return x1    def forward(self, x):        x = self.prepare_data_standard(x)        x1 = self.spatial_route(x)        x2 = self.spectral_route(x)        x1 = self.fusion_way_baseline(x1, x2)        combined_features = torch.cat([x1, x2], dim=1)        weights = self.fusion_gate(combined_features)        x = weights[:, 0:1] * x1 + weights[:, 1:2] * x2        x = self.classifier(x)        return xif __name__ == "__main__":    # 配置    input_channels = 80    num_classes = 10    patch_size = 9    batch_sizes = [2, 4, 8, 16, 32, 64, 128, 256]  # 测试不同批大小    depth = 1    # 初始化模型    model = AllinMamba(        input_channels=input_channels,        num_classes=num_classes,        patch_size=patch_size,        depth=depth    )    device = get_device()    print(f"Using device: {device}")    model = model.to(device)    # 性能测试    for batch_size in batch_sizes:        # 预热        for _ in range(10):            with torch.no_grad():                x = torch.randn(batch_size, input_channels, patch_size, patch_size).to(device)                _ = model(x)        # 计时        start_time = time.time()        for _ in range(100):            with torch.no_grad():                x = torch.randn(batch_size, input_channels, patch_size, patch_size).to(device)                out = model(x)        inference_time = (time.time() - start_time) / 100        print(f'Batch size: {batch_size}, inference time: {inference_time:.4f} s/batch')        print(f'Throughput: {batch_size / inference_time:.2f} samples/sec')