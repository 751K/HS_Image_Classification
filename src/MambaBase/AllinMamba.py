import osimport timefrom fvcore.nn import FlopCountAnalysis, flop_count_tableos.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'import torchfrom torch import nnfrom einops import rearrangefrom Train_and_Eval.device import get_device# @torch.compile(mode="reduce-overhead")class AllinMamba(nn.Module):    def __init__(self, input_channels, num_classes, patch_size, feature_dim=64, depth=1, mlp_dim=32, dropout=0.25,                 d_state=16, expand=4, mode=2, root_mamba=False):        super().__init__()        self.dim = 2        self.feature_dim = feature_dim        self.depth = depth        self.patch_size = patch_size        self.dropout = dropout        self.scan_length = patch_size // 2 * 4 + 4        self.input_channels = input_channels        self.chunk_size = 4        self.conv3d_sep = nn.Sequential(            nn.Conv3d(in_channels=1, out_channels=1, kernel_size=(7, 3, 3), padding=(3, 1, 1), groups=1),            nn.BatchNorm3d(1),            nn.SiLU(),            nn.Conv3d(in_channels=1, out_channels=8, kernel_size=1),            nn.BatchNorm3d(8),            nn.SiLU()        )        self.conv2d_sep = nn.Sequential(            nn.Conv2d(in_channels=input_channels * 8, out_channels=input_channels * 8, kernel_size=3, padding=1,                      groups=input_channels * 8),            nn.BatchNorm2d(input_channels * 8),            nn.ReLU(),            nn.Conv2d(in_channels=input_channels * 8, out_channels=feature_dim, kernel_size=1),            nn.BatchNorm2d(feature_dim),            nn.ReLU()        )        self.conv2d_dim = nn.Sequential(            nn.Conv2d(in_channels=input_channels, out_channels=feature_dim, kernel_size=1),            nn.ReLU()        )        self.linear = nn.Linear(input_channels, feature_dim)        self.fusion_gate = nn.Sequential(            nn.Linear(self.feature_dim * 2, 2),            nn.Softmax(dim=1)        )        self.classifier = nn.Sequential(            nn.Linear(feature_dim, mlp_dim),            nn.ReLU(),            nn.Dropout(dropout),            nn.Linear(mlp_dim, num_classes),            nn.BatchNorm1d(num_classes)        )        if root_mamba:            try:                from mamba_ssm import Mamba2, Mamba as Mamba1            except ImportError:                from src.MambaBase.Mamba1 import Mamba1                from src.MambaBase.Mamba2 import Mamba2        else:            from src.MambaBase.Mamba1 import Mamba1            from src.MambaBase.Mamba2 import Mamba2        # d_conv = 2/4        if mode == 1:            self.MambaLayer1 = Mamba1(                d_model=feature_dim,                d_state=d_state,                expand=expand            )            self.MambaLayer2 = Mamba1(                d_model=self.scan_length,                d_state=d_state,                expand=expand            )            self.MambaLayer3 = Mamba1(                d_model=self.input_channels,                d_state=d_state,                expand=expand            )        else:            # head 数量 = expand * d_model/ headdim。            self.MambaLayer1 = Mamba2(                d_model=feature_dim,                d_state=d_state,                headdim=16,                expand=expand,                chunk_size=self.chunk_size            )            self.MambaLayer2 = Mamba2(                d_model=self.scan_length,                d_state=d_state,                headdim=16,                expand=expand,                chunk_size=self.chunk_size            )            self.MambaLayer3 = Mamba2(                d_model=self.input_channels,                d_state=d_state,                headdim=16,                expand=expand,                chunk_size=1            )    @staticmethod    def prepare_data(x):        """        Args:            x: 原始输入数据 (B, C, H, W)        Returns:            标准化的输入数据 (B, C, H, W)        """        B, C, H, W = x.shape        x_reshaped = x.permute(0, 2, 3, 1).reshape(-1, C)        mean = x_reshaped.mean(dim=0, keepdim=True)        std = x_reshaped.std(dim=0, keepdim=True) + 1e-6        x_normalized = (x_reshaped - mean) / std        x_out = x_normalized.reshape(B, H, W, C).permute(0, 3, 1, 2)        x_out = x_out * torch.rsqrt(x_out.pow(2).mean(dim=(1, 2, 3), keepdim=True) + 1e-5)        return nn.SiLU()(x_out)    def Feature_extraction(self, x):        """        Args:            x: 输入张量, shape=(B, C, H, W)        Returns:            特征张量, shape=(B, hidden_dim, H, W)        """        residual_x = x        x = self.conv3d_sep(x.unsqueeze(1))        x = rearrange(x, 'b t c h w -> b (t c) h w')        x = self.conv2d_sep(x)        residual_x = self.conv2d_dim(residual_x)        return x + residual_x    @staticmethod    def get_weights(length, device='cuda'):        center = length // 2        weights = torch.exp(-0.5 * ((torch.arange(length) - center) / (center / 2)) ** 2).to(device)        return weights    # @staticmethod    # def get_weights(length, device):    #     weights = torch.ones(length, device=device) / length    #     return weights    def smallScan(self, x):        """        Args:            x: 输入张量, shape=(B, hidden_dim, H, W)        Returns:            螺旋扫描后的特征, shape=(B, scan_length, hidden_dim)        """        center = self.patch_size // 2        x_scan = torch.zeros((x.shape[0], x.shape[1], self.scan_length), device=x.device)        x_scan[:, :, 3] = torch.zeros((x.shape[0], x.shape[1]), device=x.device)        x_scan[:, :, 2] = torch.zeros((x.shape[0], x.shape[1]), device=x.device)        x_scan[:, :, 1] = torch.zeros((x.shape[0], x.shape[1]), device=x.device)        x_scan[:, :, 0] = x[:, :, center, center]        for i in range(center):            weights = self.get_weights(2 * i + 3, device=x.device)            # 顶行            x_slice = x[:, :, center - i - 1, center - i - 1:center + i + 2]            x_scan[:, :, i * 4 + 4] = (x_slice * weights.view(1, 1, -1)).sum(dim=-1) / weights.sum()            # 右列            x_slice = x[:, :, center - i - 1:center + i + 2, center + i + 1]            x_scan[:, :, i * 4 + 5] = (x_slice * weights.view(1, 1, -1)).sum(dim=-1) / weights.sum()            # 底行            x_slice = x[:, :, center + i + 1, center - i - 1:center + i + 2]            x_scan[:, :, i * 4 + 6] = (x_slice * weights.view(1, 1, -1)).sum(dim=-1) / weights.sum()            # 左列            x_slice = x[:, :, center - i - 1:center + i + 2, center - i - 1]            x_scan[:, :, i * 4 + 7] = (x_slice * weights.view(1, 1, -1)).sum(dim=-1) / weights.sum()        return x_scan.transpose(1, 2)    def MambaBlock(self, x):        """        Args:            x: 输入张量, shape=(B, scan_length, hidden_dim)        Returns:            经过MambaBlock处理后的张量, shape=(B, scan_length, hidden_dim)        """        for _ in range(self.depth):            x = self.MambaLayer1(x)            x = x.transpose(1, 2)            x = self.MambaLayer2(x)            x = x.transpose(1, 2)        return x    def forward(self, x):        x1 = self.spatial_route(x)        x2 = self.spectral_route(x)        gate = torch.sigmoid(x2)        x1 = gate * x1        combined_features = torch.cat([x1, x2], dim=1)        weights = self.fusion_gate(combined_features)        x = weights[:, 0:1] * x1 + weights[:, 1:2] * x2        x = self.classifier(x)        return x    def spatial_route(self, x):        x = self.prepare_data(x)        x = self.Feature_extraction(x)        x = self.smallScan(x)        x = self.MambaBlock(x)        x = x[:, 0:4].mean(dim=1)        return x    def spectral_route(self, x):        center_pixel = x[:, :, self.patch_size // 2, self.patch_size // 2]        center_pixel = center_pixel.unsqueeze(1)        for _ in range(self.depth):            center_pixel = self.MambaLayer3(center_pixel)        center_pixel = self.linear(center_pixel.squeeze(1))        return center_pixelif __name__ == "__main__":    input_channels = 80    num_classes = 10    patch_size = 9    batch_size = 4    depth = 1    # Model initialization and device placement    model = AllinMamba(input_channels=input_channels, num_classes=num_classes, patch_size=patch_size, depth=depth)    device = get_device()    print(f"Using device: {device}")    model = model.to(device)    # Inference test    x = torch.randn(batch_size, input_channels, patch_size, patch_size).to(device)    # Measure inference time and memory    start_epoch = time.time()    with torch.no_grad():  # Inference mode        out = model(x)    inference_time = time.time() - start_epoch    print(f'Inference time: {inference_time:.4f} seconds')    print(f'Output shape: {out.shape}')    # Calculate FLOPs    flops = FlopCountAnalysis(model, x)    print(flop_count_table(flops, max_depth=1))